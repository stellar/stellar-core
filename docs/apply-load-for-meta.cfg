# This is the Stellar Core configuration example for using the load generation 
# (apply-load) tool  while also writing the history and metadata. That allows load
# testing the metadata ingestion in addition to the Core itself.
# This is not meant to be used in any production contexts.

# The core with this configuration should be run using 
# `./stellar-core new-hist local && ./stellar-core apply-load`

# Custom meta path - if not set it will be written to a temp directory and 
# cleaned up after running the benchmark
METADATA_OUTPUT_STREAM='meta.xdr'

# Enable load generation
ARTIFICIALLY_GENERATE_LOAD_FOR_TESTING=true

# Diagnostic events should generally be disabled, but can be enabled for debug
ENABLE_SOROBAN_DIAGNOSTIC_EVENTS = false

# Network configuration to use during the benchmark
# The fields here correspond to the network configuration settings.
APPLY_LOAD_LEDGER_MAX_INSTRUCTIONS = 500000000
APPLY_LOAD_TX_MAX_INSTRUCTIONS = 100000000

APPLY_LOAD_LEDGER_MAX_DEPENDENT_TX_CLUSTERS = 1

APPLY_LOAD_TX_MAX_FOOTPRINT_SIZE = 200

APPLY_LOAD_LEDGER_MAX_DISK_READ_LEDGER_ENTRIES = 1000
APPLY_LOAD_TX_MAX_DISK_READ_LEDGER_ENTRIES = 100

APPLY_LOAD_LEDGER_MAX_DISK_READ_BYTES = 200000
APPLY_LOAD_TX_MAX_DISK_READ_BYTES = 130000

APPLY_LOAD_LEDGER_MAX_WRITE_LEDGER_ENTRIES = 1000
APPLY_LOAD_TX_MAX_WRITE_LEDGER_ENTRIES = 100

APPLY_LOAD_LEDGER_MAX_WRITE_BYTES = 300000
APPLY_LOAD_TX_MAX_WRITE_BYTES = 140000

APPLY_LOAD_MAX_LEDGER_TX_SIZE_BYTES = 270000
APPLY_LOAD_MAX_TX_SIZE_BYTES = 150000

APPLY_LOAD_MAX_CONTRACT_EVENT_SIZE_BYTES = 10000
APPLY_LOAD_MAX_SOROBAN_TX_COUNT = 1000

# The following section contains various parameters for the generated load.

# Number of ledgers to close for benchmark
APPLY_LOAD_NUM_LEDGERS = 100

# Generate that many simple Classic payment transactions in every benchmark ledger
APPLY_LOAD_CLASSIC_TXS_PER_LEDGER = 1000

# Size of every synthetic data entry generated.
# This setting affects both the size of the pre-generated Bucket List entries,
# and the size of every entry that a Soroban transaction reads/writes.
APPLY_LOAD_DATA_ENTRY_SIZE = 300

# Bucket list pre-generation

# The benchmark will pre-generate ledger entries using the simplified ledger
# close process; the generated ledgers won't be reflected in the meta or
# history checkpoints.

# Faster settings, more shallow BL (up to level 6)
# Number of ledgers to close
APPLY_LOAD_BL_SIMULATED_LEDGERS = 10000
# Write a batch of entries every that many ledgers
APPLY_LOAD_BL_WRITE_FREQUENCY = 1000
# Write that many entries in every batch
APPLY_LOAD_BL_BATCH_SIZE = 1000
# Write entry batches in every ledger of this many last ledgers
APPLY_LOAD_BL_LAST_BATCH_SIZE = 100
# Write that many entries in every 'last' ledger
APPLY_LOAD_BL_LAST_BATCH_LEDGERS = 300

# Slower settings, deeper BL (up to level 9)
#APPLY_LOAD_BL_SIMULATED_LEDGERS = 300000
#APPLY_LOAD_BL_WRITE_FREQUENCY = 10000
#APPLY_LOAD_BL_BATCH_SIZE = 10000
#APPLY_LOAD_BL_LAST_BATCH_LEDGERS = 300
#APPLY_LOAD_BL_LAST_BATCH_SIZE = 100

# Settings for generated transactions
# Every setting consists of the list of the possible values and the respective 
# _DISTRIBUTION list that defines the weight of every value. The values are then
# sampled from the value list according to the distribution.

# Core will try to pack as many generated transactions as possible,
# so if it's important to maintain a constant number of transactions per ledger,
# or to maintain constant utilization of every resources dimension in every
# ledger, then sampling should be avoided.

# It's generally a good idea to utilize as many dimensions as possible, so
# the values here should be chosen carefully such that the ratio between the
# generated value and the respective limit is the roughly same for most of the
# resources.

# Number of *disk* reads a transaction performs. Every disk read is restoration,
# so it's also a write (accounted for in NUM_RW_ENTRIES).
APPLY_LOAD_NUM_DISK_READ_ENTRIES = [1]
APPLY_LOAD_NUM_DISK_READ_ENTRIES_DISTRIBUTION = [1]

# Number of writes a transaction performs.
APPLY_LOAD_NUM_RW_ENTRIES = [4]
APPLY_LOAD_NUM_RW_ENTRIES_DISTRIBUTION = [1]

# Number of events a transaction emits.
APPLY_LOAD_EVENT_COUNT = [5]
APPLY_LOAD_EVENT_COUNT_DISTRIBUTION = [1]

# Size of the generated transaction.
APPLY_LOAD_TX_SIZE_BYTES = [1080]
APPLY_LOAD_TX_SIZE_BYTES_DISTRIBUTION = [1]

# Number of instructions a transaction will use.
APPLY_LOAD_INSTRUCTIONS = [2000000]
APPLY_LOAD_INSTRUCTIONS_DISTRIBUTION = [1]


# Minimal core config boilerplate

RUN_STANDALONE=true
NODE_IS_VALIDATOR=false
UNSAFE_QUORUM=true
NETWORK_PASSPHRASE="Standalone Network ; February 2017"
NODE_SEED="SDQVDISRYN2JXBS7ICL7QJAEKB3HWBJFP2QECXG7GZICAHBK4UNJCWK2 self"

[QUORUM_SET]
THRESHOLD_PERCENT=100
VALIDATORS=["$self"]

# Local history configuration to use for catching up to the synthetic
# ledger state generated before the apply load benchmark is executed.
[HISTORY.local]
get="cp -r history/{0} {1}"
put="cp -r {0} history/{1}"
mkdir="mkdir -p history/{0}"