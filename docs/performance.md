# Performance Evaluation

To gain a sense of stellar-core scalability beyond its current demands, we conducted black box performance evaluations of the implementation. The only thing our tests relied on was that ledgers close every 5 seconds (a configuration choice) and that the testing infrastructure could detect when nodes were not keeping up with each other or with the incoming transactions.

## Test setup

### Methodology

An infrastructure with configurable number of stellar-core nodes was setup, using SQLite as a backing store. We avoided PostgreSQL, as it performs some tasks asynchronously, introducing more noise in our measurements.
The following parameters were defined for our testing infrastructure:
* Total number of accounts `NB_ACCOUNTS` in the ledger. Accounts are created before running the test by performing transactions that create accounts, populating the SQL database and the bucket list.
* Transaction rate `TX_RATE`, in transactions per second. A transaction is generated by creating a payment operation between two random accounts from the ledger.
* Number of validators on the network `NB_VALIDATORS`. We configured every validator to know about every other validator (a worst-case scenario for SCP), with quorum slices set to any simple majority of nodes (51% threshold).


To perform these tests, we enabled the `generateload` endpoint, a development feature through which core can be instructed to generate synthetic load.
We created entirely new stellar-core instances for each test, to avoid any side effects from previous tests.
Transactions were injected from one of the test nodes by using `generateload` endpoint. Every test ran for 1,000 seconds. We ran each test twice, capturing four metrics:

* `scp.timing.nominated` - Time from nomination start to first prepare ("Nomination")
* `scp.timing.externalized` - Time from first prepare to value externalizing ("Balloting")
* `ledger.ledger.close` -  Time for externalizing the value post consensus ("Ledger Update")
* `ledger.transaction.count` - Confirmed transactions per ledger

Libmedida library timers were used to gather metrics. Note that those use exponentially decaying reservoirs, which are representative of roughly last five minutes of data. Additionally, 
we collected raw latencies to understand the distribution better (since libmedida does not let us see the distribution). The discoveries of this experiment are described later in this document. 

### Test network instantiation

Our testbed consisted of AWS `i2.xlarge instances`, each with:
* 32 GB RAM
* 4 vCPU
   * Each vCPU (Virtual CPU) is a hardware hyperthread on an Intel(R) Xeon(R) Processor E5-2670 v2 @ 2.50 GHz
* 1x 800 GB SSD

Instances were created on the same VPC in the same EC2 region.

## Baseline performance

### Test parameters

* `NB_ACCOUNTS = 100,000`
* `TX_RATE = 100`
* `NB_NODES = 4`

#### Nomination time
Nomination timing per slot completes within 83 ms over 90% of the time, with occasional spikes of about 1 second, corresponding to the first step in the timeout function in the nomination protocol.

#### Ballot protocol time
Similarly, the ballot protocol takes 60 ms over 90% of the time, with a few outliers at 750ms. Outliers in that stage also correspond to the ballot protocol experiencing delayed message delivery.

#### Transactions per ledger
We also noted that number of transactions per ledger stayed fairly consistent during the test. Averaging data from two runs, the mean was 483 transactions, with standard deviation of 37.5 (7.7%).
This is due to how load generator is implemented, as we confirmed that no transactions were dropped.

#### Ledger Update time
Median ledger update time was 383 ms, largely due to our choice of SQLite as a backend.

### Overall observations
We realized that focusing on percentiles is more suitable for our purposes, mostly due to outliers described above, which caused inconclusive mean and standard deviation.

## Analyzing trends

Given this baseline performance, we looked at the effects of varying each of the test setup parameters.

### Varying Transaction Rate

The transaction rate impacts:
* Number of transactions flooded over the network
* Number of transactions included in each ledger
* Size of the top level buckets

#### Test parameters

* `NB_ACCOUNTS = 100,000`
* `TX_RATE = 100..250`, in increments of 10
* `NB_NODES = 4`

#### Overall observations
We observed linear growth trend in all three metrics of interest (Nomination, Balloting, Ledger Update) as we linearly increased transaction rate.
Growth rates for nomination and ballot protocols were relatively slow (0.56 and 0.78 respectively), while ledger update time growth rate was 4.4.
We can then conclude that, not surprisingly, ledger update time is the bottleneck as we increase the load, while keeping number of accounts and topology constant.

### Varying the number of nodes on the network

Changing the number of nodes impacts:
* the number of SCP messages exchanged
* the number of potential values during nomination

#### Test parameters

* `NB_ACCOUNTS = 100,000`
* `TX_RATE = 5,10,20,100`
* `NB_NODES = 4..30`

#### Nomination, Balloting time
Nomination time median time grew linearly, at a rate smaller than the ballot protocol.

#### Ledger Update time
Ledger update time median for given `TX_RATE` was constant as expected. Additionally, as we experimented with different `TX_RATE`s, we observed that ledger
update time varied accordingly, confirming that ledger update time is mostly affected by transaction rate, and is indeed independent of the number of validators.


#### Overall observations

As shown in the graphs, we see an overall linear increase of the timings caused by network topology.

### Varying the number of accounts

Changing the number of accounts impacts:
* Overhead of interacting with SQL
* Overhead of merging buckets as buckets are larger

#### Test parameters

* `NB_ACCOUNTS = 100,000...40,000,000`
* `TX_RATE = 100`
* `NB_NODES = 4`

#### Nomination time

While we did not detect a particular pattern with nomination timing (which might be due to database, network activities), we observed that Nomination time consisently stayed under 500 ms. 

#### Balloting time

We observed that ballot protocol time remained constant and very minimal. 

#### Ledger Update time

With minuscule fluctuations, ledger update time remained constant, so we can conclude that it does not have a significant impact as number of accounts increases.

#### Overall observations

Comparing to other dimensions we tested, we see minimal impact to SCP or ledger update times, and mostly measuring noise from our shared hosting environment.

## Future Work
We could get a fairly good idea of the various trends within a couple orders
of magnitude of today's network.
Due to limitations to our test setup, we could not stress the system as much
as we'd like. In particular, we ran into problems trying to create a very large
number of accounts that we can attribute to the client library that is used for
interfacing with sql: creating accounts with the sql console worked fine.
As part of this work, we did not perform any performance optimization on STrade's
implementation, yet identified a few promising areas of investment that would yield
significant performance improvements:

* The way transactions are propagated over the network is very naive as its only purpose
is to ensure that transactions get included during nomination. The current implementation
attempts to propagate transactions to all nodes on the network, independently of SCP.

* The way values (transaction sets) used by SCP messages are propagated over the network
is also naive as seen with the trend lines on the ballot protocol, and would benefit from
algorithms such as DHT.

* Batching of write operations to the SQL database should yield significant improvements
on the ability for validators to close ledgers as the current implementation uses several layers
of nested SQL transactions that comes at a high cost.

## Useful Links

Spreadsheet with data and graphs collected:
https://docs.google.com/spreadsheets/d/1GBXwCOLyfhpgFXM5keRsGsIP6Q5GoWhFKTMxfM8qtTQ

Relevant graphs:
https://docs.google.com/document/d/1L79P_LYbxkCWeIhvVtSSVAJ0pVn6TFhVM--Au9tXHcE

Jenkins job server, where jobs can re-configured and re-run:
https://jenkins.stellar-ops.com/

[NOTE: currently the changes are in separate branches, waiting to be merged into master; Remove this note once 
master and Jenkins are updated]
Stellar-core commander used to orchestrate the tests:
https://github.com/stellar/stellar_core_commander

Test repo where new benchmark recipes were added:
https://github.com/stellar/stellar-core-acceptance
